
### Daily Research: 2026-01-25
### Query: "deep+computer+vision"
### Found *5* relevant papers.
#===================================================#



Paper: 360Anything: Geometry-Free Lifting of Images and Videos to 360°

Authors: Ziyi Wu, Daniel Watson, Andrea Tagliasacchi et al.

Categories: cs.CV

Published: 2026-01-22 18:45 UTC

Source: Arxiv


Summary: Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geo... 

#===================================================#

Why it matters: Proposes a geometry-free framework using pre-trained diffusion transformers for lifting perspective images and videos to 360° panoramas, directly addressing deep computer vision tasks in 3D world generation.

Relevance Score: 0.81/1.00

[Read more](http://arxiv.org/abs/2601.16192v1)

#===================================================#



Paper: HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval

Authors: Zequn Xie, Xin Liu, Boyun Zhang et al.

Categories: cs.CV, cs.IR

Published: 2026-01-22 17:57 UTC

Source: Arxiv


Summary: The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from "blind" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framew... 

#===================================================#

Why it matters: Introduces HVD, a human vision-inspired model for video representation learning, which is a novel approach in deep computer vision for text-video retrieval.

Relevance Score: 0.71/1.00

[Read more](http://arxiv.org/abs/2601.16155v1)

#===================================================#



Paper: IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance

Authors: Jongwoo Park, Kanchana Ranasinghe, Jinhyeok Jang et al.

Categories: cs.RO

Published: 2026-01-22 18:57 UTC

Source: Arxiv


Summary: Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model's built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively... 

#===================================================#

Why it matters: IVRA improves visual-token relations for robot action policy using training-free hint-based guidance, leveraging deep learning techniques in computer vision (image patches) for spatial understanding.

Relevance Score: 0.68/1.00

[Read more](http://arxiv.org/abs/2601.16207v1)

#===================================================#



Paper: PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation

Authors: Onkar Susladkar, Tushar Prakash, Adheesh Juvekar et al.

Categories: cs.CV, cs.AI

Published: 2026-01-22 18:58 UTC

Source: Arxiv


Summary: Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete lat... 

#===================================================#

Why it matters: PyraTok is a language-aligned pyramidal tokenizer for video understanding and generation, using deep learning (video VAE) to achieve better cross-modal alignment in computer vision.

Relevance Score: 0.68/1.00

[Read more](http://arxiv.org/abs/2601.16210v1)

#===================================================#



Paper: Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning

Authors: Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin et al.

Categories: cs.AI, cs.RO

Published: 2026-01-22 18:09 UTC

Source: Arxiv


Summary: Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Pol... 

#===================================================#

Why it matters: Cosmos Policy fine-tunes video models (a deep computer vision task) for visuomotor control and planning, leveraging pre-trained video models for practical applications.

Relevance Score: 0.68/1.00

[Read more](http://arxiv.org/abs/2601.16163v1)

#===================================================#


Additional takeaways:
Trending categories: cs.CV, cs.RO, cs.AI, cs.IR
Sources count:arxiv: 5 papers

Most active fields: cs.CV"

#===================================================#          

This post is AI-generated. 2026-01-25 14:10 UTC*
